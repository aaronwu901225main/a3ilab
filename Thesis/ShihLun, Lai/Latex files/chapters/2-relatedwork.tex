\chapter{Related Work}
\label{chapter:relatedwork}

\section{Diffusion Model on Image Synthesis}
In the field of image synthesis, diffusion models [\cite{ho2020denoising}, \cite{song2020score}, \cite{mirza2014conditional}, \cite{sohl2015deep}] have achieved state-of-the-art performance. They have broken the long-standing dominance of GAN \cite{goodfellow2020generative} in image synthesis domain. Diffusion model have demonstrated outstanding performance in various domain of image synthesis, from the unconditional diffusion models \cite{ho2020denoising} to conditional diffusion models \cite{dhariwal2021diffusion}. Diffusion models have a wide range of applications, such as Image Super-Resolution [\cite{li2022srdiff}, \cite{saharia2022image}, \cite{rombach2022high}], Inpainting [\cite{rombach2022high}, \cite{saharia2022palette}], Image translation [\cite{ozbey2023unsupervised}, \cite{saharia2022palette}] and Editing [\cite{meng2021sdedit}]. These methods all implement the diffusion model, employing different conditional inputs and modifications to the architecture based on the conditional input. This allows the diffusion model to generate corresponding images based on different conditions. Text-to-Image generation is currently one of the most frequently discussed applications of diffusion models. Diffusion models need to generate corresponding image from a descriptive text. \cite{rombach2022high} utilized a pretrained text encoder CLIP \cite{radford2021learning} and used cross-attention to combine text information with diffusion model, generating real and diverse image based on text description. On the other hand, DALLE-2 \cite{ramesh2022hierarchical} proposed a two-state approach, involving a prior model capable of generating a CLIP-based image embedding based on a text caption, and a diffusion-based decoder that can generate an image conditioned on the produced image embedding. Imagen \cite{saharia2022photorealistic} employed a different text encoder T5 \cite{raffel2020exploring} which is a text-to-text transformer, they explored that a large language model trained on only text data is still very effective on text-to-image generation. In order to train with high-resolution images, \cite{rombach2022high} shifted the diffusion process to latent space to reduce computation resources. They first employed a pretrained autoencoder to encode images into latent space, then the diffusion process and the sampling process are conducting in the latent spaces. This method effectively reduces computational resources and can be applied to various tasks.


\section{Classifier-free guidance}
The original diffusion models were unconditional, meaning they couldn't be controlled by text or categories to generate images.\cite{dhariwal2021diffusion} proposed the classifier guidance method. During the training of the diffusion model, an additional classifier is trained on noisy samples. When sampling images, the diffusion model can be guided and generate images according to the class by leveraging the gradients from the classifier. This allows the diffusion model to generate images based on the specified class. This method need to train an additional classifier, the sampling quality is highly correlated with the classifier. \cite{ho2022classifier} proposed the proposed classifier-free guidance, allowing the diffusion model to generate images according to categories without the need for an additional classifier. By combining unconditional input and conditional input with a guidance scale $\omega$, the diffusion model can generate images based on categories without relying on an additional classifier.


\section{Compositional Zero-shot Learning (CZSL)}
Traditional Zero-Shot Learning (ZSL) [\cite{wang2019survey}, \cite{pourpanah2022review}, \cite{xian2018zero}, \cite{rahman2018unified}, \cite{xian2018feature}, \cite{han2021contrastive}] investigates classification tasks when there are no training samples available, but the semantics of new classes are known. One branch of research within ZSL is Compositional Zero-Shot Learning (CZSL) [\cite{xu2021zero}, \cite{yang2022decomposable}, \cite{lu2023decomposed}, \cite{saini2022disentangling}]. Given a known object concept, people can combine combine this object with various attribute to create different compositions. CZSL represents all attributes and objects as concept are available during training phase, the model needs to recognize the unseen compositions based on seen concepts and correctly classify them during validation or testing. \cite{chen2014inferring} trained a linear classifier of seen compositions and inferred unseen compositions form observed classifier using Bayesian framework. \cite{misra2017red} propose a simple approach which decomposed a complex concepts into multiple primitives and trained a binary linear classifier on each of these primitive. This method demonstrates our ability to infer a complex concept composed of two or more primitives, which motivate us on image selection framework. \cite{nagarajan2018attributes} propose a model based on the  concept of 'attributes as operators,' representing attribute-object composition. This model conditions transformations on attributes, incorporating it into an embedding learning model for identifying unseen compositions. \cite{yang2022decomposable} create a model that learn three spaces for the object, attribute and composition classifications.


