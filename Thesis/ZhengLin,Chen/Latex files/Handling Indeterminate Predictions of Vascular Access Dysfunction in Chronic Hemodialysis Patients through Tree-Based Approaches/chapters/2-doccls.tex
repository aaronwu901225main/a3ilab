\chapter{Related Works}
\label{chapter:doccls}

\section{KDOQI Guidelines}

The 2006 update to the KDOQI guidelines~\cite{KDOQI} offers a comprehensive framework for managing vascular access in hemodialysis patients, addressing aspects from patient evaluation to long-term access maintenance. This update, based on studies conducted between 1997 and 2005, aims to reduce vascular access-related morbidity and improve long-term functionality. The guidelines include evidence-based recommendations for creating and maintaining arteriovenous fistulas (AVFs) and grafts (AVGs), highlighting specific thresholds for vascular flow rates: AVFs with flow rates below 500 mL/min and AVGs below 600 mL/min should be further evaluated for potential stenosis or dysfunction.

However, relying solely on these flow rate thresholds may not be sufficient for optimal decision-making regarding interventions. Recent studies have explored the integration of artificial intelligence (AI) and machine learning techniques to enhance surgical decision-making processes. For instance, AI-enabled clinical decision~\cite{Balch_2024} support systems have shown promise in improving surgical outcomes by analyzing complex datasets to predict patient-specific risks and recommend tailored interventions.These advancements suggest that incorporating AI into clinical practice could augment traditional guidelines, leading to more precise and individualized patient care.

In summary, while the KDOQI guidelines provide a solid foundation for vascular access management in hemodialysis patients, integrating AI and machine learning approaches holds the potential to further refine surgical decision-making, thereby enhancing patient outcomes.

\section{Physical Examination}

Regular vascular access blood flow (Qa) surveillance~\cite{Wu}, combined with routine clinical monitoring and physical examination, has proven effective in predicting vascular access stenosis in chronic hemodialysis patients. By analyzing data from 397 patients with arteriovenous fistulas (AVF) and arteriovenous grafts (AVG), this approach identifies optimal Qa thresholds for detecting stenosis. Absolute thresholds of <500 mL/min for AVF and <600 mL/min for AVG demonstrate high predictive performance, achieving accuracy rates of 91.54\% and 72.15\%, respectively. Additionally, a stricter threshold of <400 mL/min for AVF is proposed as an alternative criterion for enhanced specificity. These absolute thresholds outperform relative thresholds, which incorporate both absolute Qa levels and a 25\% decline from previous measurements, underscoring the reliability of absolute benchmarks in clinical decision-making.

Integrating these thresholds with physical examinations addresses the inherent limitations of clinical monitoring alone. While AVF patients benefit significantly from this combined approach, challenges remain for AVG patients due to structural differences in grafts, necessitating supplementary diagnostic tools such as duplex ultrasound. This framework not only improves early stenosis detection but also highlights the value of multidisciplinary collaboration in vascular access management. Furthermore, it establishes a foundation for adopting advanced techniques, such as machine learning models, to enhance prediction accuracy and inform timely interventions.
\newpage
\section{Tree-based Approaches}

Tree-based machine learning models, such as Decision Trees, Random Forests, and XGBoost, represent foundational and advanced techniques widely used in structured data analysis. These methods have evolved to balance interpretability, computational efficiency, and predictive accuracy, addressing key challenges in traditional machine learning approaches.

Decision Trees~\cite{decision_tree} form the basis of tree-based methods, utilizing recursive splitting to categorize or predict outcomes. Despite their simplicity and high interpretability, Decision Trees are prone to overfitting, especially in complex, high-dimensional datasets. To address this limitation, ensemble techniques like Random Forests~\cite{random_forest} and XGBoost~\cite{XGBoost} have emerged. Random Forests employ bagging (bootstrap aggregating), which involves training multiple decision trees on random subsets of data and aggregating their outputs. This method significantly reduces variance and enhances model stability, particularly in noisy or imbalanced datasets~\cite{comprehensive_tree_based_model}.

In contrast, XGBoost exemplifies the boosting paradigm, where weak learners are iteratively combined to minimize errors from previous iterations~\cite{Comparative_XGBoost}. XGBoost introduces advanced features like sparsity-aware algorithms and regularized objectives, making it highly efficient and scalable for large datasets. These innovations have positioned XGBoost as a dominant method in machine learning competitions and real-world applications, from clinical diagnostics to financial modeling~\cite{Yan_2020}. Recent comparative studies also highlight advancements in gradient boosting frameworks, such as LightGBM~\cite{LightGBM} and CatBoost~\cite{CatBoost}. While LightGBM excels in training speed through selective sampling techniques, CatBoost addresses prediction shifts caused by categorical feature encoding, further enhancing generalization performance. Together, these methods provide a comprehensive suite for structured data analysis, offering adaptability across diverse domains.

Tree-based models, such as XGBoost and Random Forests, often outperform neural networks on tabular data due to the unique characteristics of these datasets and the strengths of tree-based approaches. Tabular data typically consists of a mix of numerical and categorical features, irregular patterns, and skewed or heavy-tailed distributions, which tree-based models handle efficiently by learning piecewise constant functions. This allows them to capture irregular patterns, ignore irrelevant features, and adapt to data heterogeneity without extensive preprocessing or feature engineering. In contrast, neural networks struggle with these properties, requiring significant regularization and feature transformation to achieve comparable results.

Research supports these observations. Grinsztajn et al. (2022)~\cite{tree_based_model} demonstrate that tree-based models excel on medium-sized datasets (~10,000 samples), where neural networks often fail to generalize due to their reliance on smooth functions and their sensitivity to uninformative features. McElfresh and Khandagale (2023)~\cite{Outperform_Boosted_Trees} further highlight that Gradient Boosted Decision Trees (GBDTs) outperform neural networks on average, particularly in handling data irregularities and dataset-specific properties. Additionally, Ye et al. (2024)~\cite{Closer_Look} find that while deep learning has improved, tree-based models remain competitive, especially in scenarios with limited data or computational resources. These advantages, combined with their computational efficiency and minimal hyperparameter tuning requirements, make tree-based models a robust and practical choice for structured datasets, where neural networks' strengths in image and text data do not translate effectively.



\section{Ensemble Learning}

Ensemble learning has become a cornerstone in machine learning for enhancing model robustness and improving predictive accuracy by combining multiple base classifiers~\cite{9893798}. Two primary ensemble strategies, bagging and boosting, form the foundation of many successful machine learning models. Bagging methods, such as Random Forest, focus on reducing variance by training multiple classifiers independently and aggregating their outputs. This approach ensures stability and robust performance, particularly in datasets with noise or complex distributions.

In this study, we leverage the strengths of these ensemble methods by employing a soft voting classifier that combines Decision Trees, Random Forests, and XGBoost~\cite{Kim2018AutomatedML, Interpretable_tree_ensemble}. The soft voting approach integrates predictions probabilistically, assigning weights based on the confidence levels of individual models. This method captures both the diversity and the complementary strengths of the base classifiers, ensuring a more balanced and accurate prediction framework.

The decision to use a soft voting ensemble stems from its proven ability to manage structured data effectively. Unlike neural networks, which may struggle with limited structured datasets and interpretability, tree-based ensembles offer clear advantages in handling imbalanced data, feature interactions, and hierarchical relationships inherent in clinical and structured datasets.

\section{Indeterminacy Handling and Interpretability}

In this study, tree-based models were employed to address challenges in structured data classification by integrating Indeterminacy quantification and enhancing model interpretability. While the methods used are specific to tree-based models, the approach is inspired by concepts and techniques explored in other machine learning and deep learning research~\cite{Uncertainty_Quantification}.

Indeterminacy quantification is a critical aspect of this study, as it enables the model to effectively manage ambiguous cases and provide more reliable predictions. Insights from Indeterminacy modeling~\cite{Uncertainty_Quantification_2}  provide a foundational framework, offering methodologies to quantify both aleatoric and epistemic indeterminacies and evaluate their impact on predictive accuracy. These concepts guided the implementation of metrics and approaches tailored for tree-based models to improve their reliability and adaptability in structured data classification.

Another key influence is the use of Indeterminacy-based rejection techniques~\cite{electronics11030396}, which demonstrate the utility of quantifying prediction Indeterminacy to enhance model selection and enable the integration of diverse classifiers. Inspired by these strategies, this study incorporates mechanisms to reallocate highly indeterminate predictions to an "indeterminate" category, ensuring more cautious decision-making. This approach is particularly valuable in clinical applications where incorrect classifications could lead to significant consequences.

The focus on interpretability in this study also draws from methods that emphasize local perturbations to approximate complex model behavior~\cite{perturbations}, providing interpretable explanations for individual predictions. These techniques influenced the design of Indeterminacy-aware explanations for tree-based models, facilitating a deeper understanding of decision boundaries and feature contributions for each prediction.

Lastly, the study integrates principles inspired by methods that estimate Indeterminacy using randomness in model computations~\cite{pmlr-v48-gal16}. Although initially designed for neural networks, these concepts informed the multipass perturbation approach employed here, where noise is introduced into input features to analyze prediction variability. This method enables the approximation of Indeterminacy in ensemble tree-based models, extending the applicability of these ideas to structured data classification.

This integrated framework leverages existing advancements in Indeterminacy quantification and interpretability to adapt them for tree-based models, enabling robust and explainable predictions in complex real-world scenarios.