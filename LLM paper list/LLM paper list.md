Here is the list of LLM papers we plan to study.

# LLM

* Zhang, Hanqing, et al. "A survey of controllable text generation using transformer-based pre-trained language models." ACM Computing Surveys (2022).[Paper](https://dl.acm.org/doi/10.1145/3617680)

* Shumailov, Ilia, et al. "The Curse of Recursion: Training on Generated Data Makes Models Forget." arXiv preprint arxiv:2305.17493 (2023).[Paper](https://arxiv.org/abs/2305.17493)

* Pan, Shirui, et al. "Unifying Large Language Models and Knowledge Graphs: A Roadmap." arXiv preprint arXiv:2306.08302 (2023).[Paper](https://arxiv.org/abs/2306.08302)

* Wong, Lionel, et al. "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought." arXiv preprint arXiv:2306.12672 (2023).[Paper](https://arxiv.org/abs/2306.12672)

* Kaddour, Jean, et al. "Challenges and applications of large language models." arXiv preprint arXiv:2307.10169 (2023).[Paper](https://arxiv.org/abs/2307.10169)

* Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." International Conference on Machine Learning. PMLR, 2021.[Paper](https://proceedings.mlr.press/v139/zhao21c.html)

* Liang, Percy, et al. "Holistic evaluation of language models." arXiv preprint arXiv:2211.09110 (2022).[Paper](https://arxiv.org/abs/2211.09110)

* Algorithmic prompting or how to teach math to a large language model
[Paper](https://the-decoder.com/how-to-teach-math-to-a-large-language-model/)



### LLM - Reasoning

* Zheng, Shen, Jie Huang, and Kevin Chen-Chuan Chang. "Why Does ChatGPT Fall Short in Answering Questions Faithfully?." arXiv preprint arXiv:2304.10513 (2023).[Paper](https://arxiv.org/abs/2304.10513)

* Zhang, Hanlin, et al. "Improved logical reasoning of language models via differentiable symbolic programming." arXiv preprint arXiv:2305.03742 (2023).[Paper](https://arxiv.org/abs/2305.03742)

* Yao, Shunyu, et al. "React: Synergizing reasoning and acting in language models." arXiv preprint arXiv:2210.03629 (2022).[Paper](https://arxiv.org/abs/2210.03629)

* Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." arXiv preprint arXiv:2305.10601 (2023).[Paper](https://arxiv.org/abs/2305.10601)

* Poesia, Gabriel, et al. "Certified Reasoning with Language Models." arXiv preprint arXiv:2306.04031 (2023).[Paper](https://arxiv.org/abs/2306.04031)

* Huang, Yue, Qihui Zhang, and Lichao Sun. "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models." arXiv preprint arXiv:2306.11507 (2023).[Paper](https://arxiv.org/abs/2306.11507)

* Lee, Harrison, et al. "Rlaif: Scaling reinforcement learning from human feedback with ai feedback." arXiv preprint arXiv:2309.00267 (2023).[Paper](https://arxiv.org/abs/2309.00267)

* Huang, Jie, et al. "Large Language Models Cannot Self-Correct Reasoning Yet." arXiv preprint arXiv:2310.01798 (2023).[Paper](https://arxiv.org/abs/2310.01798)

* Goyal, Sachin, et al. "Think before you speak: Training Language Models With Pause Tokens." arXiv preprint arXiv:2310.02226 (2023).[Paper](https://arxiv.org/abs/2310.02226?trk=public_post_comment-text)

* Zhou, Andy, et al. "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models." arXiv preprint arXiv:2310.04406 (2023).[Paper](https://arxiv.org/abs/2310.04406)

* Kadavath, Saurav, et al. "Language models (mostly) know what they know." arXiv preprint arXiv:2207.05221 (2022).[Paper](https://arxiv.org/abs/2207.05221)



#### LLM - Reasoning - Math

* Poesia, Gabriel, and Noah D. Goodman. "Peano: learning formal mathematical reasoning." Philosophical Transactions of the Royal Society A 381.2251 (2023): 20220044.[Paper](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2022.0044)

* Scarlatos, Alexander, and Andrew Lan. "Tree-based representation and generation of natural and mathematical language." arXiv preprint arXiv:2302.07974 (2023).[Paper](https://arxiv.org/abs/2302.07974)

* Dziri, Nouha, et al. "Faith and Fate: Limits of Transformers on Compositionality." arXiv preprint arXiv:2305.18654 (2023).[Paper](https://arxiv.org/abs/2305.18654)

* Yang, Zhen, et al. "GPT Can Solve Mathematical Problems Without a Calculator." arXiv preprint arXiv:2309.03241 (2023).[Paper](https://arxiv.org/abs/2309.03241)

* Yu, Longhui, et al. "Metamath: Bootstrap your own mathematical questions for large language models." arXiv preprint arXiv:2309.12284 (2023).[Paper](https://arxiv.org/abs/2309.12284)

* Gou, Zhibin, et al. "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving." arXiv preprint arXiv:2309.17452 (2023).[Paper](https://arxiv.org/abs/2309.17452)



### RLHF

* Dong, Yi, et al. "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF." arXiv preprint arXiv:2310.05344 (2023).[Paper](https://arxiv.org/abs/2310.05344)

### Theorem

* Dai, Damai, et al. "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers." ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023.[Paper](https://openreview.net/forum?id=fzbHRjAd8U)

### Dataset curation

* Xu, Can, et al. "Wizardlm: Empowering large language models to follow complex instructions." arXiv preprint arXiv:2304.12244 (2023).[Paper](https://arxiv.org/abs/2304.12244)

* Mukherjee, Subhabrata, et al. "Orca: Progressive learning from complex explanation traces of gpt-4." arXiv preprint arXiv:2306.02707 (2023).[Paper](https://arxiv.org/abs/2306.02707)

  
### Efficient computtion

* Dao, Tri, et al. "Flashattention: Fast and memory-efficient exact attention with io-awareness." Advances in Neural Information Processing Systems 35 (2022): 16344-16359.[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html)

* Csordás, Róbert, Kazuki Irie, and Jürgen Schmidhuber. "Approximating Two-Layer Feedforward Networks for Efficient Transformers." arXiv preprint arXiv:2310.10837 (2023).[Paper](https://arxiv.org/abs/2310.10837)

* Zhangir Azerbayev. "Llemma: An Open Language Model For Mathematics" arXiv preprint arXiv:2310.10631 (2023).[Paper](https://arxiv.org/abs/2310.10631)

* Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." arXiv preprint arXiv:2305.14314 (2023).[Paper](https://arxiv.org/abs/2305.14314)


### Financial

* Liu, Xiao-Yang, Guoxuan Wang, and Daochen Zha. "FinGPT: Democratizing Internet-scale Data for Financial Large Language Models." arXiv preprint arXiv:2307.10485 (2023).[Paper](https://arxiv.org/abs/2307.10485)

* de Padua, Raul Salles, Imran Qureshi, and Mustafa U. Karakaplan. "GPT-3 Models are Few-Shot Financial Reasoners." arXiv preprint arXiv:2307.13617 (2023).[Paper](https://arxiv.org/abs/2307.13617)

### History and Forcasting

* Krenn, Mario, et al. "Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network." Nature Machine Intelligence (2023): 1-10.[Paper](https://www.nature.com/articles/s42256-023-00735-0)

### Localized LLM

* Huang, Huang, et al. "AceGPT, Localizing Large Language Models in Arabic." arXiv preprint arXiv:2309.12053 (2023).[Paper](https://arxiv.org/abs/2309.12053)

### Long input

* Ding, Jiayu, et al. "Longnet: Scaling transformers to 1,000,000,000 tokens." arXiv preprint arXiv:2307.02486 (2023).[Paper](https://arxiv.org/abs/2307.02486)


### Planning and Dcision Making

* Dagan, Gautier, Frank Keller, and Alex Lascarides. "Dynamic Planning with a LLM." arXiv preprint arXiv:2308.06391 (2023).[Paper](https://arxiv.org/pdf/2308.06391.pdf)

### Prompt Engineering

* Yang, Chengrun, et al. "Large language models as optimizers." arXiv preprint arXiv:2309.03409 (2023).[Paper](https://arxiv.org/abs/2309.03409)

* Shi, Zhengxiang, and Aldo Lipani. "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning." arXiv preprint arXiv:2309.05173 (2023).[Paper](https://arxiv.org/abs/2309.05173)

* ChatGPT Prompt Engineering ChatGPT提示工程 [Paper](file:///C:/Users/a4691/OneDrive/Desktop/LLM/Prompt%20Engineering/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8D%97%20_%20Prompt%20Engineering%20Guide.html)





