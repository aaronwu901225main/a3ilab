Here is the list of LLM papers we plan to study.

# LLM

Zhang, Hanqing, et al. "A survey of controllable text generation using transformer-based pre-trained language models." ACM Computing Surveys (2022).[Paper](https://dl.acm.org/doi/10.1145/3617680)

Shumailov, Ilia, et al. "The Curse of Recursion: Training on Generated Data Makes Models Forget." arXiv preprint arxiv:2305.17493 (2023).[Paper](https://arxiv.org/abs/2305.17493)

Pan, Shirui, et al. "Unifying Large Language Models and Knowledge Graphs: A Roadmap." arXiv preprint arXiv:2306.08302 (2023).[Paper](https://arxiv.org/abs/2306.08302)

Wong, Lionel, et al. "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought." arXiv preprint arXiv:2306.12672 (2023).[Paper](https://arxiv.org/abs/2306.12672)

Kaddour, Jean, et al. "Challenges and applications of large language models." arXiv preprint arXiv:2307.10169 (2023).[Paper](https://arxiv.org/abs/2307.10169)

Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." International Conference on Machine Learning. PMLR, 2021.[Paper](https://proceedings.mlr.press/v139/zhao21c.html)

Liang, Percy, et al. "Holistic evaluation of language models." arXiv preprint arXiv:2211.09110 (2022).[Paper](https://arxiv.org/abs/2211.09110)

Algorithmic prompting or how to teach math to a large language model
[Paper](https://the-decoder.com/how-to-teach-math-to-a-large-language-model/)



## LLM - Reasoning

Zheng, Shen, Jie Huang, and Kevin Chen-Chuan Chang. "Why Does ChatGPT Fall Short in Answering Questions Faithfully?." arXiv preprint arXiv:2304.10513 (2023).[Paper](https://arxiv.org/abs/2304.10513)

### LLM - Reasoning - Math

## RLHF

## Theorem

paper 3 ...

paper 4 ...
