\begin{abstract}%

In typical machine learning tasks, the test dataset usually shares the same distribution as the training dataset. Similarly, in the domain of image generation, the generation models often produce data with the same distribution as the training dataset. We addressed this limitation by employing various methods to achieve unseen data generation using a simple example. When existing methods failed to accomplish this, we further decomposed unseen data generation into unseen compositional image generation. We guided the generation model using compositional class labels. Unlike traditional approaches that use a single class label to guide the generation process, our model uses multiple category labels, such as attribute categories and object categories, to control the generation process. Through training, the model learns features from different types of labels to achieve compositional zero-shot image generation.

Regarding zero-shot generation, large language models like GPT-4 and large-scale image generation models like DALL-E 2 also possess similar zero-shot generation capabilities. However, our research focuses on implementing zero-shot generation within specific domains and using smaller models. To demonstrate the potential and prerequisites of compositional zero-shot image generation as a task achievable with contemporary machine learning techniques, we designed a series of tasks ranging from simple to complex compositional zero-shot generation tasks. In the simpler tasks, our proposed method accurately generates new compositional samples. In the more complex tasks, the newly generated compositional samples from our model, when selected appropriately, also yield reasonable new samples.

\vspace{17cm}

Keyword: Image generation, Diffusion Model, Compositional zero-shot image generation, Compositional zero-shot learning 

\end{abstract}