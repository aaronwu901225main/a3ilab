Here is the list of LLM papers we plan to study.

# LLM

Zhang, Hanqing, et al. "A survey of controllable text generation using transformer-based pre-trained language models." ACM Computing Surveys (2022).[Paper](https://dl.acm.org/doi/10.1145/3617680)

Shumailov, Ilia, et al. "The Curse of Recursion: Training on Generated Data Makes Models Forget." arXiv preprint arxiv:2305.17493 (2023).[Paper](https://arxiv.org/abs/2305.17493)

Pan, Shirui, et al. "Unifying Large Language Models and Knowledge Graphs: A Roadmap." arXiv preprint arXiv:2306.08302 (2023).[Paper](https://arxiv.org/abs/2306.08302)

Wong, Lionel, et al. "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought." arXiv preprint arXiv:2306.12672 (2023).[Paper](https://arxiv.org/abs/2306.12672)

Kaddour, Jean, et al. "Challenges and applications of large language models." arXiv preprint arXiv:2307.10169 (2023).[Paper](https://arxiv.org/abs/2307.10169)

Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." International Conference on Machine Learning. PMLR, 2021.[Paper](https://proceedings.mlr.press/v139/zhao21c.html)

Liang, Percy, et al. "Holistic evaluation of language models." arXiv preprint arXiv:2211.09110 (2022).[Paper](https://arxiv.org/abs/2211.09110)

Algorithmic prompting or how to teach math to a large language model
[Paper](https://the-decoder.com/how-to-teach-math-to-a-large-language-model/)



## LLM - Reasoning

Zheng, Shen, Jie Huang, and Kevin Chen-Chuan Chang. "Why Does ChatGPT Fall Short in Answering Questions Faithfully?." arXiv preprint arXiv:2304.10513 (2023).[Paper](https://arxiv.org/abs/2304.10513)

Zhang, Hanlin, et al. "Improved logical reasoning of language models via differentiable symbolic programming." arXiv preprint arXiv:2305.03742 (2023).[Paper](https://arxiv.org/abs/2305.03742)

Yao, Shunyu, et al. "React: Synergizing reasoning and acting in language models." arXiv preprint arXiv:2210.03629 (2022).[Paper](https://arxiv.org/abs/2210.03629)

Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." arXiv preprint arXiv:2305.10601 (2023).[Paper](https://arxiv.org/abs/2305.10601)

Poesia, Gabriel, et al. "Certified Reasoning with Language Models." arXiv preprint arXiv:2306.04031 (2023).[Paper](https://arxiv.org/abs/2306.04031)

Huang, Yue, Qihui Zhang, and Lichao Sun. "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models." arXiv preprint arXiv:2306.11507 (2023).[Paper](https://arxiv.org/abs/2306.11507)

Lee, Harrison, et al. "Rlaif: Scaling reinforcement learning from human feedback with ai feedback." arXiv preprint arXiv:2309.00267 (2023).[Paper](https://arxiv.org/abs/2309.00267)

Huang, Jie, et al. "Large Language Models Cannot Self-Correct Reasoning Yet." arXiv preprint arXiv:2310.01798 (2023).[Paper](https://arxiv.org/abs/2310.01798)

Goyal, Sachin, et al. "Think before you speak: Training Language Models With Pause Tokens." arXiv preprint arXiv:2310.02226 (2023).[Paper](https://arxiv.org/abs/2310.02226?trk=public_post_comment-text)

Zhou, Andy, et al. "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models." arXiv preprint arXiv:2310.04406 (2023).[Paper](https://arxiv.org/abs/2310.04406)

Kadavath, Saurav, et al. "Language models (mostly) know what they know." arXiv preprint arXiv:2207.05221 (2022).[Paper](https://arxiv.org/abs/2207.05221)



### LLM - Reasoning - Math

## RLHF

## Theorem

paper 3 ...

paper 4 ...
